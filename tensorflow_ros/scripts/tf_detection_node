#!/usr/bin/env python

# System
import os
import operator
import sys

# ROS
import rospy
import rosservice

from sensor_msgs.msg import Image

# OpenCV
from cv_bridge import CvBridge, CvBridgeError
import cv2

import numpy as np

import datetime
import uuid

# object recognition
import object_tracking_msgs
from object_tracking_msgs.srv import DetectObjects
from object_tracking_msgs.msg import ObjectLocation, Hypothesis, ObjectShape
from object_tracking_msgs.msg import CategoryProbability
from image_recognition_util import image_writer_test_gui
from tensorflow_ros import detector
from tensorflow_ros import recognizer

class TensorflowDetectionNode:
    """ Performs object detection using Tensorflow neural networks """
    def __init__(self, graph_path, labels_path, rec_path, save_images_folder,num_classes=99,detection_threshold=0.5,
                 image_topic="/pepper_robot/sink/front/image_raw",show_results=True):
        """ Constructor
        :param graph_path: string with path + filename (incl. extension) indicating the database location
        :param labels_path: string with path + filename (incl. extension) indicating the location of the text file
        with labels etc.
        :param save_images_folder: Where to store images for debugging or data collection
        """

        # Check if the parameters are correct
        if not (os.path.isfile(graph_path) and os.path.isfile(labels_path)):
            err_msg = "DB file {} or models file {} does not exist".format(graph_path, labels_path)
            rospy.logerr(err_msg)
            sys.exit(err_msg)
        if not (int(num_classes) > 0):
            err_msg = "invalid number of classes".format(num_classes)
            rospy.logerr(err_msg)
            sys.exit(err_msg)

        self._unknown_prob = 0.1
        self.do_segmentation = False
        self.segmentation_result = False

        #init detector
        self.detector = detector.Detector(num_classes, detection_threshold)
        self.recognizer = recognizer.Recognizer()

        self._objectLocationList = []  # List with Detections s
        self._bridge = CvBridge()
        self._detect_srv = rospy.Service('detect', DetectObjects, self._detect_srv_callback)
        self._do_detection = False  # Indicates whether a new request has been received and thus detection must
        # be performed

        self.seg_service_name = "/segment"
        if self.seg_service_name in rosservice.get_service_list():
            self._srv_segment = rospy.ServiceProxy(self.seg_service_name, rosservice.get_service_class_by_name(self.seg_service_name))
        else:
            self._srv_segment = None

        self._filename = "/tmp/tf_obj_detect.jpg"  # Temporary file name
        self._size = {'width': 0, 'height': 0}
        self._save_images_folder = save_images_folder
        self._bgr_image = None
        self.cv_image = None

        self.recognition_threshold = 0.2

        self.roi_path = "/tmp/"
        graph_r = rec_path + "/output_graph.pb"
        labels_r = rec_path + "/output_labels.txt"

        rospy.loginfo("show_results: {}".format(show_results))
        self._show_results = show_results

        self._sub = rospy.Subscriber(image_topic, Image, self._image_callback)
        rospy.loginfo("Listening to %s -- spinning .." % self._sub.name)

        rospy.loginfo("TensorflowDetectionNode initialized:")
        rospy.loginfo(" - graph_path=%s", graph_path)
        rospy.loginfo(" - labels_path=%s", labels_path)
        rospy.loginfo(" - save_images_folder=%s", save_images_folder)
        rospy.loginfo(" - num_classes=%s", num_classes)
        rospy.loginfo(" - detection_threshold=%s", detection_threshold)
        rospy.loginfo(" - rec_graph=%s",graph_r)
        rospy.loginfo(" - rec_labels=%s",labels_r)

        """1. Load graph from saved GraphDef file """
        self.detector.load_graph(graph_path, labels_path)
        rospy.loginfo("detection graph successfully loaded")

        self.recognizer.load_graph(graph_r, labels_r)
        rospy.loginfo("recognition graph successfully loaded")

    def _image_callback(self, msg):
        """
        Sensor_msgs/Image callback
        :param msg: The image message
        """
        try:
            self.cv_image = self._bridge.imgmsg_to_cv2(msg, "bgr8")
        except CvBridgeError as e:
            rospy.logerr(e)

    def _detect_srv_callback(self, req):
        """ Callback function for the detection. It saves the image on a temporary location and sets _do_detection
        to True. Subsequently, it waits until the image has been processed (i.e., until _do_detection is False again)
        and then returns the result
        :param req: object_tracking_msgs.srv.DetectRequest
        :return: object_tracking_msgs.srv.DetectResponse
        """
        try:
            self._bgr_image = self._bridge.imgmsg_to_cv2(req.image, "bgr8")
        except CvBridgeError as e:
            #error_msg = "Could not convert to opencv image: %s" % e
            #rospy.logerr(error_msg)
            #raise Exception(error_msg)
            print("No image in request. Get image from rostopic.")
            self._bgr_image = self.cv_image
        size = self._bgr_image.shape[:2]
        self._size['height'] = size[0]
        self._size['width'] = size[1]
        self._objectLocationList = []
        self._do_detection = True
        self.do_segmentation = req.doSegmentation
        print("do segmentation: {}".format(self.do_segmentation))
        if not self._srv_segment:
            if self.seg_service_name in rosservice.get_service_list():
                self._srv_segment = rospy.ServiceProxy(self.seg_service_name, rosservice.get_service_class_by_name(self.seg_service_name))
            else:
                self.do_segmentation = False
                rospy.logerr("Segmentation service client not found!")


        # Wait until the request has been processed and return the result
        r = rospy.Rate(1000.0)  # Not a problem to spin quickly
        while not rospy.is_shutdown():
            if not self._do_detection:
                detectRes = {"objectLocationList": self._objectLocationList, "segmentationResult": self.segmentation_result}
                return detectRes
        # Return an empty result if rospy has been shutdown
        return {"objectLocationList": [], "segmentationResult": False}

    def update(self):
        """ Do the actual work: if _do_detection is True, it retrieves the saved image and tries to classify it.
        The result is stored in the _detection member and afterwards _do_detection is set to False. This function
        is called at a fixed frequency in the mean thread, hence NOT from the service callback. """

        if self._bgr_image is not None:
            self.show_image(self._bgr_image)

        if not self._do_detection:
            return

        # detection
        classes, scores, boxes = self.detector.detect(self._bgr_image)
        height, width, _ = self._bgr_image.shape

        # recognition based on bboxes
        labels = []
        for i in range(0,len(classes)):
            xmin = int(boxes[i][1] * width)
            xmax = int(boxes[i][3] * width)
            ymin = int(boxes[i][0] * height)
            ymax = int(boxes[i][2] * height)
            roi = self._bgr_image[ymin:ymax, xmin:xmax]
            if (abs(ymin-ymax) <= 0 or abs(xmin-xmax) <= 0):
                print "ERROR: roi size is 0"
            #TODO: directly from memory
            imgpath = "{}/img{}.jpg".format(self.roi_path, datetime.datetime.now().strftime("%Y-%m-%d-%H-%M-%S_%f"))
            cv2.imwrite(filename=imgpath, img=roi)
            #recognize
            sorted_result = self.recognizer.recognize(imgpath)
            if sorted_result:
                best_label = sorted_result[-1][0]
                best_prob = sorted_result[-1][1]
            #TODO: unknown probability hardcoded
            best = CategoryProbability(label="unknown", probability=self._unknown_prob)
            if best_prob > best.probability:
                best.label = best_label
                best.probability = best_prob
            labels.append(best.label)
            scores[i] = best.probability

        # add results
        for i in range(0,len(classes)):
            if (labels [i] == "reject"):
                continue
            # threshold for recognition
            if (scores[i] < self.recognition_threshold):
                continue

            objectLocation = ObjectLocation()
            # set uuid
            obj_uuid = uuid.uuid4()
            objectLocation.name = str(obj_uuid)
            hypothesis = Hypothesis(label=labels[i], reliability=scores[i])
            roi = boxes[i]
            objectLocation.hypotheses.append(hypothesis)
            objectLocation.bounding_box.y_offset = int(roi[0]*self._size['height'])
            objectLocation.bounding_box.x_offset = int(roi[1]*self._size['width'])
            objectLocation.bounding_box.height = int((roi[2]-roi[0])*self._size['height'])
            objectLocation.bounding_box.width = int((roi[3]-roi[1])*self._size['width'])
            self._objectLocationList.append(objectLocation)

        # show results
        if (self._show_results):
            self.visualize_bounding_boxes(self._objectLocationList, self._bgr_image)

        # do segmentation
        if (self.do_segmentation):

            # depth lookup
            depthLookupResult = []
            for obj in self._objectLocationList:
                objectShape = ObjectShape()
                objectShape.bounding_box = obj.bounding_box
                objectShape.hypotheses = obj.hypotheses
                objectShape.name = obj.name

                scale_factor = 0.001  # rgb->depth scale factor (depthLookup: 2.0, why?)
                width_factor = 1.3
                # estimate 3d location by offset of 2d image center (rgb and depth camera coordinate frames are slightly shifted!)
                x_shift = 0.09  # urdf_val * 3
                y_shift = -0.05  # urdf_val
                objectShape.center.x = (obj.bounding_box.x_offset - width / 2) * scale_factor + x_shift
                objectShape.center.y = (obj.bounding_box.y_offset - height / 2) * scale_factor + y_shift
                objectShape.center.z = 0.5
                objectShape.width = obj.bounding_box.width * scale_factor * width_factor
                objectShape.height = obj.bounding_box.height * scale_factor * width_factor
                objectShape.depth = 0.7
                depthLookupResult.append(objectShape)

            print depthLookupResult
            # call segmentation
            try:
                seg_res = self._srv_segment(depthLookupResult).done
                print(seg_res)
                self.segmentation_result = seg_res
            except Exception as e:
                rospy.logerr("Exception in segmentation service call: ")
                self.segmentation_result = False
            self.do_segmentation = False

        self._do_detection = False

    #TODO: improve visualization
    def visualize_bounding_boxes(self, result, image):
        # visualization of detection results
        toDraw = {}
        for i in range(0, len(result)):
            tmp_label = result[i].hypotheses[0].label
            if tmp_label not in toDraw:
                toDraw[tmp_label] = i
            elif result[toDraw[tmp_label]].hypotheses[0].reliability < result[i].hypotheses[0].reliability:
                toDraw[result[i].hypotheses[0].label] = i

        for i in range(0, len(result)):      #for key in toDraw:
            #i = toDraw[key]
            prob = result[i].hypotheses[0].reliability
            prob = round(prob, 2)
            label = result[i].hypotheses[0].label
            bbox = result[i].bounding_box
            cv2.rectangle(image, (int(bbox.x_offset), int(bbox.y_offset)),
                          (int(bbox.x_offset + bbox.width), int(bbox.y_offset + bbox.height)), (0, 100, 200), 2)
            image_label = '%s %s' % (label, str(prob))

            if bbox.y_offset < 40:
                cv2.putText(image, image_label, (int(bbox.x_offset), int(bbox.y_offset) + int(bbox.height)), 0, 0.8, (0, 40, 255), 3)
            else:
                cv2.putText(image, image_label, (int(bbox.x_offset), int(bbox.y_offset)), 0, 0.8, (0, 40, 255), 3)
            self.show_image(image)

    def show_image(self, image):
        cv2.imshow('tf detection', cv2.resize(image, (1280, 960)))
        cv2.waitKey(1)

    def matchBoundingBoxes(self, detected, annotated, max_ratio=2):

        # Detected vars
        xmax_det = detected.bbox.xmax
        xmin_det = detected.bbox.xmin
        ymax_det = detected.bbox.ymax
        ymin_det = detected.bbox.ymin
        width_det = xmax_det - xmin_det
        height_det = ymax_det - ymin_det
        area_det = width_det * height_det

        # Annotated vars
        xmax_an = annotated.bbox.xmax
        xmin_an = annotated.bbox.xmin
        ymax_an = annotated.bbox.ymax
        ymin_an = annotated.bbox.ymin
        width_an = xmax_an - xmin_an
        height_an = ymax_an - ymin_an
        area_an = width_an * height_an

        innerArea = max(0, min(xmax_det, xmax_an) - max(xmin_det, xmin_an)) * max(0,
                                                                                  min(ymax_det, ymax_an) - max(ymin_det,
                                                                                                               ymin_an))
        outerArea = area_an + area_det - (2 * innerArea)
        if (innerArea == 0):
            ratio = 5
        else:
            ratio = outerArea / float(innerArea)  # the smaller the better

        # relative distances
        maxDistX = area_an * 0.5
        maxDistY = area_an * 0.5
        # alternatively match centroids of bboxes
        # or evaluate overlapping area of bboxes

        if (ratio < max_ratio):
            return True

        if (inRange(xmin_det, xmin_an, maxDistX) and inRange(ymin_det, ymin_an, maxDistY)
                and inRange(xmax_det, xmax_an, maxDistX) and inRange(ymax_det, ymax_an, maxDistY)):
            return True
        else:
            return False


if __name__ == '__main__':

    # Start ROS node
    rospy.init_node('tf_detection_ros')

    try:
        _graph_path = os.path.expanduser(rospy.get_param("~graph_path"))
        _labels_path = os.path.expanduser(rospy.get_param("~labels_path"))
        num_classes = rospy.get_param("~num_classes", 99)
        detection_threshold = rospy.get_param("~detection_threshold", 0.5)
        save_images = rospy.get_param("~save_images", True)
        image_topic = rospy.get_param("~image_topic", "/pepper_robot/sink/front/image_raw")
        _rec_path = os.path.expanduser(rospy.get_param("~rec_path"))
        _show_results = rospy.get_param("~show_results", True)

        save_images_folder = None
        if save_images:
            save_images_folder = os.path.expanduser(rospy.get_param("~save_images_folder", "/tmp/tensorflow_ros"))
    except KeyError as e:
        rospy.logerr("Parameter %s not found" % e)
        sys.exit(1)

    # Create object
    object_detection = TensorflowDetectionNode(graph_path=_graph_path,
                                               labels_path=_labels_path,
                                               rec_path=_rec_path,
                                               save_images_folder=save_images_folder,
                                               num_classes=num_classes,
                                               detection_threshold=detection_threshold,
                                               image_topic=image_topic,
                                               show_results=_show_results)
    # Start update loop
    r = rospy.Rate(100.0)
    while not rospy.is_shutdown():
        object_detection.update()
        r.sleep()
